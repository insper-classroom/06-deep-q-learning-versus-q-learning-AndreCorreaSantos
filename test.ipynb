{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/cuda/lib/python3.8/site-packages/gymnasium/envs/registration.py:527: UserWarning: \u001b[33mWARN: Using the latest versioned environment `MountainCar-v0` instead of the unversioned environment `MountainCar`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch import nn\n",
    "from DeepQLearning import DeepQLearning\n",
    "from learning import Agent\n",
    "\n",
    "env = gym.make('MountainCar')\n",
    "#env.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "## pytorch model\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0],512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,env.action_space.n)\n",
    "        )\n",
    "\n",
    "criterion = nn.MSELoss()# MSE\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)# ADAM\n",
    "\n",
    "\n",
    "d_params = {\n",
    "    \"env\":env,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"epsilon\" : 1.0,\n",
    "    \"epsilon_min\" : 0.01,\n",
    "    \"epsilon_dec\" : 0.99,\n",
    "    \"episodes\" : 400,\n",
    "    \"batch_size\" : 128,\n",
    "    \"memory\" : deque(maxlen=10000), #talvez usar uma memoria mais curta\n",
    "    \"model\" : model,\n",
    "    \"criterion\" : criterion,\n",
    "    \"optimizer\" : optimizer,\n",
    "    \"max_steps\" : 500,\n",
    "}\n",
    "\n",
    "\n",
    "q_params = {\n",
    "    \"epsilon\":0.9,\n",
    "    \"alpha\":0.1,\n",
    "    \"gamma\":0.99,\n",
    "    \"e_dec\":0.999,\n",
    "    \"e_min\":0.01,\n",
    "    \"algo\":\"q-learning\",\n",
    "    \"env\":\"MountainCar-v0\",\n",
    "    \"cont\":True,\n",
    "    \"training\":False,\n",
    "    \"max_episodes\": 400}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling(rewards,window):\n",
    "    for i in range(window,len(rewards)):\n",
    "         rewards[i-window:i].mean()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0\n",
      "run 1\n",
      "run 2\n",
      "run 3\n",
      "run 4\n"
     ]
    }
   ],
   "source": [
    "q_list = []\n",
    "for i in range(5):\n",
    "    Q = Agent(**q_params)\n",
    "    Q.read_training(\"q_learning.txt\",inplace=True)\n",
    "    rewards = np.array(Q.execute())\n",
    "    q_list.append(rolling(rewards,50))\n",
    "    \n",
    "    print(f\"run {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_data_for_seaborn(d_list, q_list):\n",
    "    all_data = []\n",
    "    \n",
    "    for run_idx, run_data in enumerate(d_list):\n",
    "        for step_idx, reward in enumerate(run_data):\n",
    "            all_data.append({\n",
    "                'Algorithm': 'DQN',\n",
    "                'Run': run_idx,\n",
    "                'Step': step_idx,\n",
    "                'Reward': reward\n",
    "            })\n",
    "    \n",
    "    for run_idx, run_data in enumerate(q_list):\n",
    "        for step_idx, reward in enumerate(run_data):\n",
    "            all_data.append({\n",
    "                'Algorithm': 'Q-learning',\n",
    "                'Run': run_idx,\n",
    "                'Step': step_idx,\n",
    "                'Reward': reward\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "df = prepare_data_for_seaborn(d_list, q_list)\n",
    "\n",
    "sns.lineplot(\n",
    "    data=df,\n",
    "    x='Step',\n",
    "    y='Reward',\n",
    "    hue='Algorithm',\n",
    "    errorbar=('ci', 95),\n",
    "    palette=['blue', 'red']\n",
    ")\n",
    "\n",
    "plt.title('Training Curves: DQN vs Q-learning with 95% CI', fontsize=16)\n",
    "plt.xlabel('Training Steps (Rolling Window = 50)', fontsize=14)\n",
    "plt.ylabel('Average Reward', fontsize=14)\n",
    "plt.legend(title='Algorithm')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
